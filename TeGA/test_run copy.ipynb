{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "INPUT_FOLDER = \"/home/jiyoung.lee/ICMR2024/datasets\" # TODO: Fill the input path\n",
    "test_file = f\"{INPUT_FOLDER}/test.json\" # TODO: Change the test file if needed\n",
    "\n",
    "test_data = []\n",
    "\n",
    "with open(test_file, 'r') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            json_obj = json.loads(line)\n",
    "            test_data.append(json_obj)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Skipping invalid JSON: {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_data = []\n",
    "\n",
    "for json_obj in test_data:\n",
    "    # For task 1\n",
    "    if 'article_url' in json_obj:\n",
    "        del json_obj['article_url']\n",
    "    if 'entity_list' in json_obj:\n",
    "        del json_obj['entity_list']\n",
    "    if 'caption1_modified' in json_obj:\n",
    "        del json_obj['caption1_modified']\n",
    "    if 'caption1_entities' in json_obj:\n",
    "        del json_obj['caption1_entities']\n",
    "    if 'caption2_modified' in json_obj:\n",
    "        del json_obj['caption2_modified']\n",
    "    if 'caption2_entities' in json_obj:\n",
    "        del json_obj['caption2_entities']\n",
    "    if 'maskrcnn_bboxes' in json_obj:\n",
    "        del json_obj['maskrcnn_bboxes']\n",
    "    if 'bert_base_score' in json_obj:\n",
    "        del json_obj['bert_base_score']\n",
    "    if 'bert_large_score' in json_obj:\n",
    "        del json_obj['bert_large_score']\n",
    "\n",
    "    # For task 2\n",
    "    if 'caption_modified' in json_obj:\n",
    "        del json_obj['caption_modified']\n",
    "    if 'caption_entities' in json_obj:\n",
    "        del json_obj['caption_entities']\n",
    "\n",
    "    modified_data.append(json_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for data_dict in modified_data:\n",
    "    # For task 1\n",
    "    if 'caption1' in data_dict:\n",
    "        data_dict['caption1'] = re.sub(r'[^a-zA-Z0-9.\\s]', '', data_dict['caption1'])\n",
    "    if 'caption2' in data_dict:\n",
    "        data_dict['caption2'] = re.sub(r'[^a-zA-Z0-9.\\s]', '', data_dict['caption2'])\n",
    "\n",
    "    # For task 2\n",
    "    if 'caption' in data_dict:\n",
    "        data_dict['caption'] = re.sub(r'[^a-zA-Z0-9.\\s]', '', data_dict['caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['caption1'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = modified_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers torch diffusers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "\n",
    "sd_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    \"nitrosocke/Ghibli-Diffusion\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    safety_checker = None,\n",
    "    requires_safety_checker = False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_deviation(image1, image2):\n",
    "    image_tensor1 = clip_processor(images=image1, return_tensors=\"pt\").to(device)\n",
    "    image_tensor2 = clip_processor(images=image2, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    image_feature1 = clip_model.get_image_features(image_tensor1.pixel_values)\n",
    "    image_feature2 = clip_model.get_image_features(image_tensor2.pixel_values)\n",
    "\n",
    "    similarity = torch.nn.functional.cosine_similarity(image_feature1, image_feature2, dim=-1)\n",
    "    deviation_value = 1 - similarity.item()\n",
    "\n",
    "    return deviation_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "nli_model.to(device)\n",
    "\n",
    "def nli(string1, string2):\n",
    "    inputs = tokenizer(string1, string2, truncation=True, return_tensors=\"pt\")\n",
    "    inputs.to(device)\n",
    "\n",
    "    outputs = nli_model(**inputs)\n",
    "    prediction = torch.softmax(outputs.logits[0], -1).tolist()\n",
    "\n",
    "    label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "    prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def has_negative_difference(caption1, caption2):\n",
    "    negative_words = {'not', 'neither', 'nor'}\n",
    "\n",
    "    for word in negative_words:\n",
    "        pattern = re.compile(r'\\b' + re.escape(word) + r'\\b')\n",
    "        if pattern.search(caption1) and not pattern.search(caption2):\n",
    "            return True\n",
    "        elif pattern.search(caption2) and not pattern.search(caption1):\n",
    "            return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 추가 설치 필요\n",
    "#pip install joblib\n",
    "#pip3 install -U scikit-learn\n",
    "# pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "context_matching = joblib.load('model/context_matching_model.pkl')\n",
    "\n",
    "def deviation_predict(X):\n",
    "    return context_matching.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(data_list, file_path):\n",
    "\n",
    "    with open(file_path, 'w') as file:\n",
    "        for item in data_list:\n",
    "            file.write(str(item) + '\\n')\n",
    "\n",
    "    print(f'The list has been saved to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "def task1(item):\n",
    "    generator = torch.Generator(device=device).manual_seed(1024)\n",
    "    img_path = item['img_local_path']\n",
    "    local_path = '/home/jiyoung.lee/ICMR2024/datasets'\n",
    "    original_image = Image.open(os.path.join(local_path, img_path))\n",
    "    caption1 = item['caption1']\n",
    "    caption2 = item['caption2']\n",
    "\n",
    "    generated_image_1 = sd_pipe(prompt=caption1, image=original_image, strength=0.75, guidance_scale=7.5, generator=generator).images[0]\n",
    "    deviation_value_1 = compute_deviation(original_image, generated_image_1)\n",
    "    deviation_value_1 = np.array(deviation_value_1).reshape(-1, 1)\n",
    "    label_1 = deviation_predict(deviation_value_1)\n",
    "\n",
    "    generated_image_2 = sd_pipe(prompt=caption2, image=original_image, strength=0.75, guidance_scale=7.5, generator=generator).images[0]\n",
    "    deviation_value_2 = compute_deviation(original_image, generated_image_2)\n",
    "    deviation_value_2 = np.array(deviation_value_2).reshape(-1, 1)\n",
    "    label_2 = deviation_predict(deviation_value_2)\n",
    "\n",
    "    # Condition\n",
    "    if (label_1 + label_2 == 0):\n",
    "        label = 0\n",
    "        return label\n",
    "    else:\n",
    "        label = 1\n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "def task2(item):\n",
    "    generator = torch.Generator(device=device).manual_seed(1024)\n",
    "    img_path = item['img_local_path']\n",
    "    local_path = '/home/jiyoung.lee/ICMR2024/datasets'\n",
    "    original_image = Image.open(os.path.join(local_path, img_path))\n",
    "    caption = item['caption1']\n",
    "\n",
    "    generated_image = sd_pipe(prompt=caption, image=original_image, strength=0.75, guidance_scale=7.5, generator=generator).images[0]\n",
    "    deviation_value = compute_deviation(original_image, generated_image)\n",
    "    deviation_value = np.array(deviation_value).reshape(-1, 1)\n",
    "    label = deviation_predict(deviation_value)[0]\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, average_precision_score, f1_score\n",
    "\n",
    "def test_task1(pair):\n",
    "    prediction_list = []\n",
    "    true_labels = []\n",
    "\n",
    "    for i, item in enumerate(pair):\n",
    "        caption1 = item['caption1']\n",
    "        caption2 = item['caption2']\n",
    "        context_label = item['context_label']\n",
    "\n",
    "        nli_result = nli(caption1, caption2)\n",
    "        is_negative = has_negative_difference(caption1, caption2)\n",
    "\n",
    "        if nli_result['entailment'] >= 80:\n",
    "            prediction = 0\n",
    "        elif is_negative:\n",
    "            prediction = 1\n",
    "        else:\n",
    "            prediction = task1(item)\n",
    "\n",
    "        prediction_list.append(prediction)\n",
    "        true_labels.append(context_label)\n",
    "\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, prediction_list)\n",
    "    average_precision = average_precision_score(true_labels, prediction_list)\n",
    "    f1 = f1_score(true_labels, prediction_list)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Average Precision:\", average_precision)\n",
    "    print(\"F1-Score:\", f1)\n",
    "\n",
    "    return prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, average_precision_score, f1_score\n",
    "\n",
    "def test_task2(pair):\n",
    "    prediction_list = []\n",
    "    true_labels = []\n",
    "\n",
    "    for i, item in enumerate(pair):\n",
    "        caption = item['caption1']\n",
    "        context_label = item['context_label']\n",
    "\n",
    "        prediction = task2(item)\n",
    "\n",
    "        prediction_list.append(prediction)\n",
    "        true_labels.append(context_label)\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, prediction_list)\n",
    "    average_precision = average_precision_score(true_labels, prediction_list)\n",
    "    f1 = f1_score(true_labels, prediction_list)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Average Precision:\", average_precision)\n",
    "    print(\"F1-Score:\", f1)   \n",
    "\n",
    "    return prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excute Task1 Code \n",
    "prediction_list = test_task1(pair)\n",
    "output_file = \"task1.txt\"\n",
    "save_file(prediction_list, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excute Task2 Code\n",
    "prediction_list = test_task2(pair)\n",
    "output_file = \"task2.txt\"\n",
    "save_file(prediction_list, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tega",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
